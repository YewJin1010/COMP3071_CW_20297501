Scenario: Lunar Lander Landing Zone Selection

User Input: You get to choose the landing zone for the lunar lander. Specify the x-coordinate (horizontal position) where you want the lander to touch down (between -1 and 1).
Agent Behavior:
PPO: PPO is an on-policy algorithm that directly optimizes the policy. It tends to explore more cautiously and adapt to the chosen landing zone.
DQN: DQN is an off-policy algorithm that learns from a replay buffer. It may take more aggressive actions and explore widely.
A2C: A2C combines value-based and policy-based methods. It balances exploration and exploitation, aiming for stable performance.
Results:
Run each agent with your chosen landing zone and observe:
How quickly they learn to land safely.
Whether they overshoot or undershoot the target.
How stable their landings are.
Feel free to provide an x-coordinate (between -1 and 1)


Scenario: Fuel Efficiency Challenge

User Input: You have a limited amount of fuel for the lunar lander. Specify the maximum fuel capacity (between 100 and 500 units).
Agent Behavior:
PPO: PPO aims to optimize the policy while considering fuel efficiency. It will try to land the lander using the least amount of fuel.
DQN: DQN learns from experience and may not prioritize fuel efficiency initially. It might explore different trajectories, including fuel-intensive ones.
A2C: A2C balances exploration and exploitation. It will likely find a trade-off between fuel conservation and successful landings.
Results:
Run each agent with the specified fuel capacity and observe:
How efficiently they use fuel during descent.
Whether they prioritize fuel conservation or landing accuracy.
How their strategies differ based on the fuel constraint.
Feel free to provide the maximum fuel capacity

